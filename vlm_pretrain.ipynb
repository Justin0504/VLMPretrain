{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_w2PAYa-wr4"
      },
      "source": [
        "# VLM_Pretrain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLfRSt-U-wr6"
      },
      "source": [
        "### Requirements\n",
        "\n",
        "- Python 3.10.16\n",
        "- CUDA 12.2\n",
        "- [requirements.txt](./requirements.txt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9mmaOD8CN9l"
      },
      "source": [
        "\n",
        "### Start Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zquG6dYbCX4J"
      },
      "source": [
        "\n",
        "#### Step 0: Download the clip model and the language model\n",
        "```bash\n",
        "# Download the clip model to the ./model/vision_model directory\n",
        "git clone https://huggingface.co/openai/clip-vit-base-patch16\n",
        "# or\n",
        "git clone https://www.modelscope.cn/models/openai-mirror/clip-vit-base-patch16\n",
        "```\n",
        "\n",
        "```bash\n",
        "# Download the pure language model weights to the ./out directory (as the base language model for training VLM)\n",
        "https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch/blob/main/lm_512.pth\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UP9UFu-wr6"
      },
      "source": [
        "#### Step 1: Install the dependencies\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Uo0rps-wr7"
      },
      "source": [
        "#### Step 2: Prepare the dataset\n",
        "\n",
        "Download the dataset from [here](https://huggingface.co/datasets/jingyaogong/minimind-v_dataset) and put it in the `./dataset` directory.\n",
        "\n",
        "```\n",
        "git clone https://huggingface.co/datasets/jingyaogong/minimind-v_dataset\n",
        "```\n",
        "\n",
        "`*.jsonl` is the question-answer dataset, `*images` is the corresponding image data, and the image data needs to be decompressed after downloading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT4ZlU7HCDtL"
      },
      "source": [
        "#### Step 3: Run the code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "XO23-EoA-wr8",
        "outputId": "3f7abb0b-9b6b-4add-da2c-825ef2e01a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment setup completed!\n"
          ]
        }
      ],
      "source": [
        "# First import the necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import time\n",
        "import math\n",
        "from contextlib import nullcontext\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Add path\n",
        "sys.path.append('..')\n",
        "from model.model_vlm import MiniMindVLM, VLMConfig\n",
        "from dataset.lm_dataset import VLMDataset\n",
        "\n",
        "print(\"Environment setup completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_M5UK7RM-wr8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "Training epochs: 1\n",
            "Batch size: 24\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.epochs = 1\n",
        "        self.batch_size = 24\n",
        "        self.learning_rate = 4e-4\n",
        "        self.accumulation_steps = 1\n",
        "        self.grad_clip = 1.0\n",
        "        self.log_interval = 100\n",
        "        self.save_interval = 1000\n",
        "\n",
        "        # Model configuration\n",
        "        self.hidden_size = 512\n",
        "        self.num_hidden_layers = 8\n",
        "        self.max_seq_len = 640\n",
        "        self.use_moe = False\n",
        "\n",
        "        # Data paths\n",
        "        self.data_path = \"dataset/pretrain_data.jsonl\"\n",
        "        self.images_path = \"dataset/pretrain_images\"\n",
        "        self.save_dir = \"out\"\n",
        "\n",
        "args = Config()\n",
        "print(f\"Using device: {args.device}\")\n",
        "print(f\"Training epochs: {args.epochs}\")\n",
        "print(f\"Batch size: {args.batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dajqD1cn-wr9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VLM trainable parameters: 0.394M\n",
            "Model initialization completed!\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "def init_model():\n",
        "    # Create model configuration\n",
        "    model_config = VLMConfig(\n",
        "        hidden_size=args.hidden_size,\n",
        "        num_hidden_layers=args.num_hidden_layers,\n",
        "        max_seq_len=args.max_seq_len\n",
        "    )\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('model/vision_model/clip-vit-base-patch16', use_fast=True)\n",
        "\n",
        "    # Create VLM model\n",
        "    model = MiniMindVLM(model_config, vision_model_path=\"model/vision_model/clip-vit-base-patch16\")\n",
        "\n",
        "    # Load pre-trained language model weights\n",
        "    moe_path = '_moe' if model_config.use_moe else ''\n",
        "    ckp = f'{args.save_dir}/lm_{model_config.hidden_size}{moe_path}.pth'\n",
        "    state_dict = torch.load(ckp, map_location=args.device)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    # Only train vision_proj layer, freeze other parameters\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'vision_proj' not in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6\n",
        "    print(f'VLM trainable parameters: {trainable_params:.3f}M')\n",
        "\n",
        "    _, preprocess = model.vision_encoder, model.processor\n",
        "    return model.to(args.device), tokenizer, preprocess, model_config\n",
        "\n",
        "# Initialize\n",
        "model, tokenizer, preprocess, model_config = init_model()\n",
        "print(\"Model initialization completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eXo1-jR4-wr9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 595375\n",
            "Number of batches: 24808\n",
            "Data loader and optimizer setup completed!\n"
          ]
        }
      ],
      "source": [
        "# Create dataset and data loader\n",
        "train_ds = VLMDataset(\n",
        "    args.data_path,\n",
        "    args.images_path,\n",
        "    tokenizer,\n",
        "    preprocess=preprocess,\n",
        "    image_special_token=model_config.image_special_token,\n",
        "    max_length=args.max_seq_len\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=args.batch_size,\n",
        "    pin_memory=True,\n",
        "    drop_last=False,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(train_ds)}\")\n",
        "print(f\"Number of batches: {len(train_loader)}\")\n",
        "\n",
        "# Initialize training components\n",
        "os.makedirs(args.save_dir, exist_ok=True)\n",
        "ctx = nullcontext() if args.device == \"cpu\" else torch.cuda.amp.autocast()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(args.device != \"cpu\"))\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.learning_rate)\n",
        "\n",
        "print(\"Data loader and optimizer setup completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UiNX-h_U-wr-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training function definition completed!\n"
          ]
        }
      ],
      "source": [
        "# Helper functions\n",
        "def get_lr(current_step, total_steps, lr):\n",
        "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
        "\n",
        "# Simplified training function\n",
        "def train_step(X, Y, loss_mask, pixel_values, step, epoch, total_steps):\n",
        "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Adjust learning rate\n",
        "    lr = get_lr(step, total_steps, args.learning_rate)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # Forward pass\n",
        "    with ctx:\n",
        "        res = model(X, pixel_values=pixel_values)\n",
        "        loss = loss_fct(\n",
        "            res.logits.view(-1, res.logits.size(-1)),\n",
        "            Y.view(-1)\n",
        "        ).view(Y.size())\n",
        "\n",
        "        loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
        "        loss += res.aux_loss\n",
        "        loss = loss / args.accumulation_steps\n",
        "\n",
        "    # Backward pass\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    # Gradient update\n",
        "    if (step + 1) % args.accumulation_steps == 0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    return loss.item(), lr\n",
        "\n",
        "print(\"Training function definition completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycgZ1N9H-wr-"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"=\" * 50)\n",
        "print(\"Starting VLM pre-training\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "model.train()\n",
        "iter_per_epoch = len(train_loader)\n",
        "total_steps = args.epochs * iter_per_epoch\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for step, (X, Y, loss_mask, pixel_values) in enumerate(train_loader):\n",
        "        # Move data to device\n",
        "        X = X.to(args.device)\n",
        "        Y = Y.to(args.device)\n",
        "        loss_mask = loss_mask.to(args.device)\n",
        "        pixel_values = pixel_values.to(args.device)\n",
        "\n",
        "        # Training step\n",
        "        global_step = epoch * iter_per_epoch + step\n",
        "        loss, lr = train_step(X, Y, loss_mask, pixel_values, global_step, epoch, total_steps)\n",
        "        epoch_loss += loss\n",
        "\n",
        "        # Print logs\n",
        "        if step % args.log_interval == 0 and step > 0:\n",
        "            spend_time = time.time() - start_time\n",
        "            avg_loss = epoch_loss / (step + 1)\n",
        "            # Calculate estimated time remaining for the epoch\n",
        "            steps_remaining_epoch = iter_per_epoch - (step + 1)\n",
        "            time_per_step = spend_time / (step + 1)\n",
        "            estimated_time_epoch = steps_remaining_epoch * time_per_step\n",
        "\n",
        "            # Calculate estimated total time remaining\n",
        "            steps_remaining_total = total_steps - global_step - 1\n",
        "            estimated_time_total = estimated_time_total = steps_remaining_total * time_per_step\n",
        "\n",
        "            print(f'Epoch:[{epoch+1}/{args.epochs}]({step}/{iter_per_epoch}) '\n",
        "                  f'loss:{loss:.3f} avg_loss:{avg_loss:.3f} lr:{lr:.7f} '\n",
        "                  f'Epoch ETA: {estimated_time_epoch:.2f}s Total ETA: {estimated_time_total:.2f}s')\n",
        "\n",
        "        # Save model\n",
        "        if (step + 1) % args.save_interval == 0:\n",
        "            model.eval()\n",
        "            moe_path = '_moe' if model_config.use_moe else ''\n",
        "            ckp = f'{args.save_dir}/pretrain_vlm_{model_config.hidden_size}{moe_path}.pth'\n",
        "            state_dict = model.state_dict()\n",
        "            # Only save non-vision_encoder parameters\n",
        "            clean_state_dict = {\n",
        "                key: value for key, value in state_dict.items()\n",
        "                if not key.startswith('vision_encoder.')\n",
        "            }\n",
        "            clean_state_dict = {k: v.half() for k, v in clean_state_dict.items()}\n",
        "            torch.save(clean_state_dict, ckp)\n",
        "            print(f\"Model saved to: {ckp}\")\n",
        "            model.train()\n",
        "\n",
        "    # Statistics after each epoch\n",
        "    epoch_time = time.time() - start_time\n",
        "    avg_loss = epoch_loss / iter_per_epoch\n",
        "    print(f'Epoch {epoch+1} completed! Average loss: {avg_loss:.3f}, Time: {epoch_time:.2f}s')\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEI1Z-DO-wr-"
      },
      "outputs": [],
      "source": [
        "# Final model save\n",
        "model.eval()\n",
        "moe_path = '_moe' if model_config.use_moe else ''\n",
        "final_ckp = f'{args.save_dir}/pretrain_vlm_{model_config.hidden_size}{moe_path}_final.pth'\n",
        "\n",
        "state_dict = model.state_dict()\n",
        "clean_state_dict = {\n",
        "    key: value for key, value in state_dict.items()\n",
        "    if not key.startswith('vision_encoder.')\n",
        "}\n",
        "clean_state_dict = {k: v.half() for k, v in clean_state_dict.items()}\n",
        "torch.save(clean_state_dict, final_ckp)\n",
        "\n",
        "print(f\"Final model saved to: {final_ckp}\")\n",
        "print(\"All training processes completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRXcXdNV-wr-"
      },
      "outputs": [],
      "source": [
        "# Try to chat with the model\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from transformers import TextStreamer\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "def chat_with_vlm(prompt, pixel_values, image_names):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    new_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )[-args.max_seq_len + 1:]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        new_prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True\n",
        "    ).to(args.device)\n",
        "\n",
        "    print(f'[Image]: {image_names}')\n",
        "    print('ü§ñÔ∏è: ', end='')\n",
        "    generated_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=args.max_seq_len,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        streamer=streamer,\n",
        "        top_p=0.95,\n",
        "        temperature=0.6,\n",
        "        pixel_values=pixel_values\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "    print('\\n\\n')\n",
        "\n",
        "\n",
        "image_dir = './dataset/eval_images/'\n",
        "prompt = f\"{model.params.image_special_token}\\nPlease describe the content of the image.\"\n",
        "\n",
        "for image_file in os.listdir(image_dir):\n",
        "    image = Image.open(os.path.join(image_dir, image_file)).convert('RGB')\n",
        "    pixel_tensors = MiniMindVLM.image2tensor(image, preprocess).to(args.device).unsqueeze(0)\n",
        "    chat_with_vlm(prompt, pixel_tensors, image_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C20pzQQBE5Mc"
      },
      "source": [
        "## Reference\n",
        "\n",
        "- [Minimind-v](https://github.com/jingyaogong/minimind-v)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
